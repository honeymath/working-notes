# AI 失控不是模型问题，而是系统边界的失守

## 前言

2023 年第一次接触 ChatGPT 时，一种高度自动化的生活出现我脑海：  
我用最松散的语言随意说话，chatGPT像一个小助理一样帮我思考一切完成一切。我只要好好躺在床上我说想要干嘛，他就能又快又准的帮我做好。

2023 年底，GPT Actions 出现，我开始构建自己的 agent：读邮件、发邮件、汇总回复、辅助安排事务。我尝试让它来帮我安排每位speaker的演讲时间。但问题很快显现——它开始胡编乱造。甚至出现一大堆基础层面的常识性失误。那种体验非常清晰：我不是在使用一个智能工具，而是在不断纠正一个不断会犯滑稽可笑的错误的学徒。  究竟是我在苦口婆心教导一个孩子，还是先进的人工智能在帮我完成任务？最终，日程安排只能由我亲自完成。从那一刻起，我也对 AI 失去了最基本的信任。

2025 年，模型能力显著提升。过去那种明显的幻觉大幅减少。但即便如此，我依然不敢让它替我发邮件。  
在**孤立、低风险的小任务**中，它已经足够好用；但一旦进入**长流程、大系统**，问题立刻变得本质化：  
如果它出错了，代价由谁承担？  
如果错误被悄然传播，谁能第一时间察觉？

这个问题，并不会因为模型变强而自动消失。

---

## 一个正在被加速推进、但尚未完成约束设计的系统

当下的人工智能发展，更像一场只有油门、没有刹车的赛车比赛。
早在 ChatGPT 刚落地时，围绕安全机制、对齐研究与系统风险的讨论就已大量出现，但这些议题始终难以与速度、规模和效率享有同等的设计优先级。这是因为在竞争环境中，投入时间研究“安全带”和“刹车系统”，短期内并不会带来性能指标的显著提升；而引擎和油箱仍有巨大扩展空间，任何在约束和安全上投入的时间，都会让团队在性能竞赛中迅速落后于只专注“加速”的对手。
这种激励结构自然形成了一种囚徒困境，推动整个行业在尚未完成安全与约束设计的前提下持续加速。
理论上，当引擎逼近上限时，注意力应当转向制动系统；但在现实中，只要尚未发生足够严重的事故，这种惯性几乎不可能自行中断。

在投资视角下，这种趋势被进一步放大。
任何与 AI 挂钩的叙事都会持续获得奖励，人们对技术前景的判断高度依赖 scaling law，却有意或无意地忽视了数据枯竭、能源成本与系统复杂性的上限。
“AI 训练 AI”“模型蒸馏降低成本”等设想被反复强化，而 AI 系统自身的不可预测性、级联失败与放大效应却很少被纳入同等严肃的讨论。

在学术研究中，对模型性能、benchmark 以及 chain-of-thought 的关注本身并无问题；
真正的偏移发生在传播过程中——当“多展示几步推理过程”被简化、包装为“更接近人类思考”的证据时，推理的稳定性、可重复性、失败模式与可验证性却被悄然隐去。

在工程实践中，依托 transformer 架构所带来的“高并发”能力，系统开始以近乎无约束的方式吞纳来自现实世界的大规模数据；与此同时，LLM 被嵌入具备持续状态、环境感知与动作选择能力的 agent 系统中，用语言推理接管系统的决策与流程演化。这类操作被包装为一种“新范式”，并迅速整合进各类工程系统之中。
与此同时，既有的理论、方法与工程经验被过早地视为“过时”，仿佛可以绕过历史积累，直接完成范式替换——例如，将 LLM 设想为“操作系统”，让用户只与模型对话来完成一切操作，从而取代原有的 UI 与交互范式。
在这种叙事中，成功案例被反复放大、不断许诺未来，而失败案例却极少进入公共视野，更缺乏对其失败机制的系统性分析。

更值得警惕的，是一种长期存在却很少被明确指出的语言中心主义假设：
将语言能力视为元能力，并据此将认知、推理、规划与行动统一建立在语言之上。

由于人类高度依赖语言进行协作、反思与创造，当 AI 学会使用人类语言与我们对话时，人们便自然推断它也应当具备理解、判断与创造的能力，仿佛智能的顶点就是“像我们一样说话”。
这种推论隐含着一个危险的错误前提：人类语言可以承载并替代一切能力。

但人类语言只是载体，而非认知本身。

能讲故事不等于理解故事；

表达流畅不等于推理稳定, 更不意味着它能为自己生成的结论承担责任。

这些技术假设、商业激励与社会焦虑相互叠加，迅速演化为一场仓促的“AI 转型运动”。
在尚未明确界定 AI 是什么、适合做什么、以及在哪些场景下不应被使用之前，许多团队已被要求将 AI 作为“未来生产力的必选项”强行引入，而相应的失败责任划分、可靠性检查与退出机制却并未同步建立。
上层施压但下层难以执行的张力，使AI在组织内部存在的实际形态、功能和期待都开始扭曲。

社会整体因此呈现出一种矛盾状态：
在希望中夹杂恐慌，在兴奋中逐步失控。

在公共话语场中，一些声音甚至借机断言：未来的孩子不再需要学习编程，因为终将被替代。
这种说法看似前瞻，实则回避了最关键的问题——
真正的风险不在于“AI 会不会取代人”，而在于我们是否愿意在对失败与错误零容忍的领域中，放心地嵌入一个会胡说八道、无法定位错误、也无法承担责任的黑盒系统。

---
## 一个冷静的思考——LLM 到底是什么，它创造了什么，能力又从何而来？

从技术源头看，今天主流的大语言模型所依赖的注意力机制，最初并非为了“对话”或“通用智能”，它最早在机器翻译任务中被系统化提出，其核心作用是显式建模序列内部及跨序列的对应关系。
在自回归训练目标与大规模数据训练的配合下，这一机制被纳入语言模型体系之中，使模型整体逐渐展现出一种稳定而高效的能力——语义补全。

在这一意义上，LLM 所做的事情从未改变，本质只有两类：
翻译（在不同符号系统之间建立映射），以及
补全（在给定上下文中延续模式与结构）。

翻译降低了语言体系之间的摩擦；补全使模型能够在上下文中延续结构、语义与形式。这两者叠加，制造出一种“类似推理”“近似通用”的外观——即便其中始终伴随着不可忽视的幻觉。

第一层真正具有颠覆性的能力，确实来自翻译。
LLM 将自然语言转化为可执行文本，使系统能够通过自然语言被控制、被修改状态、被自动化，甚至直接运行命令。这一变化并非自然语言“进化”出了执行力，而是程序语言原本具备的执行能力，经由翻译被迁移了过来。

关键不在于“能不能做”，而在于翻译成本被压低甚至消除。
当不同语言体系之间的隔离被打破，原本封闭在工程系统中的能力开始流通、重组，并向外显现。

Agent 的出现正是这一过程的自然外溢：
需求 → 语言 → 代码 → 执行，这条链路第一次被压缩进单一系统之中。
因此，LLM 首当其冲冲击的并非“创造性劳动”，而是翻译型岗位；而低端程序开发，本质上同样是一种翻译——将模糊的人类意图映射为形式化、可执行的指令。

然而，对从未接触过编程或形式化语言的公众而言，这种变化具有极强的感知冲击。

他们看到的并不是“程序能力被翻译到了自然语言中”，
而是日常语言突然获得了执行力：
命令被理解，意图被落实，结果被直接产出，不再需要显式代码、中介系统或专业训练。

这种体验在直觉上极其反常。
它很容易被误读为“硅基生命的觉醒”，仿佛某种脱离人类设计的智能正在自发进化。
但事实上，执行能力始终存在，只是长期被封装在工程体系内部，以渐进、对外不可见的方式演化。
正是这种长期不可见的积累与突然显性的外观之间的断裂，制造了当下的集体震撼。

第二层令人震撼的能力来自补全。
通过对海量文本的学习，LLM 将大量知识与经验压缩进参数空间，使其在面对“填空式”问题时，效率远高于人类逐条检索资料的方式。它并不消除错误，但显著降低了试错的时间成本。

补全并未赋予语言新的属性，而是放大了语言原本就具备的功能。

逻辑从何而来？——自然语言本身就携带结构化逻辑。

通用性从何而来？——自然语言本身就是通用的交流工具。

条件、因果、量化、指代、递归，早已嵌入语言结构之中；补全机制只是延续这些结构。因此，LLM 在语言层面呈现出“通用”“可规划”“可推理”的外观。

但这种能力严格受限于三点：
语言本身的表达边界；
训练分布中曾出现过的结构；
以及上下文窗口内可被维持的约束关系。

它并不存在一个独立于语言之外的推理系统，也无法保证结论在语义之外依然成立。
当这种语言层面的延续被误认为是可迁移、可审计、可保证的推理能力时，幻觉便产生了。

至此，问题已较为清晰：
LLM 的能力确实“来自语言”，但更准确地说——

LLM 本身并不创造这些能力，而只是在不同语言体系之间打通了一条隧道，将其他语言体系中早已存在的能力，引入仅通过单一语言进行交互的主体的可见范围之内。

当下的诸多风险亦正源于此：语言在完成能力引入的同时，模糊了不同语言体系原本清晰的边界，使安全、责任与验证问题不再局限于局部结构，而被整体扩散至整个系统之中。

---

## 但是，这条路并没有走错

尽管围绕语言与智能的误解频繁出现，这条技术路径本身并未走偏。  
语言与文字作为人类文明中存在了数千乃至上万年的工具，其有效性早已在演化与历史中被反复验证。

文字与符号可以承载形式逻辑、支持数学推理；  
它们构成交流与协作的基础，奠定法律文本与制度设计的可能；  
社会的组织、物种的延续、文明的兴衰，从未脱离语言这一载体。

但语言本身并不是“智能的来源”。  
它更像是一种**形式载体**，如同纸张、记号。真正发挥作用的，是其背后那套按照逻辑、约束与规则运转的系统。

在人类自身内部，这一点同样成立。  
语言中枢与逻辑控制中枢并非同一机制；  
人的整体行为并不是由语言单独驱动，而是一个高度耦合、分层协作的系统。  
我们常误以为自己“用语言思考”，但更准确的说法是：  
**我们的思考过程恰好可以被语言表达出来。**

不同语言天然具备不同的功能与限制，这一点在数学中尤为明显。  
一个设计良好的符号系统，往往不仅提高推理效率，甚至能够直接暴露问题的本质。

数学推理与证明在形式层面上，本质是**变量替换与结构匹配**的过程。  
符号系统的设计优劣，直接决定了这种匹配是否稳定、可组合、可迁移。

以微积分为例。  
牛顿的“点”记号在表达变化率时极其直观，但一旦涉及复杂换元或嵌套结构，符号本身不再携带足够的信息，推理容易失控；  
相比之下，莱布尼茨显式保留变量关系的记法虽然冗长，却在结构迁移与组合推理中表现出更强的鲁棒性。

考虑链式法则：  
设 $$f(x) = g(h(x))$$。  
如果仅使用点记号 $$ f', g', h' $$，就必须**额外引入一条外在规则**：
$$f'(x) = g'(h(x)) \cdot h'(x).$$
这一规则并不从符号本身自然显现，而是依赖记忆与约定。

而在莱布尼茨记号下，推理过程则直接体现在符号形态之中：
$$\frac{df}{dx}
= \frac{df}{dh} \cdot \frac{dh}{dx}
= \frac{dg}{dh} \cdot \frac{dh}{dx}
= g'(h(x)) \cdot h'(x). $$
这里，**变量替换本身即构成系统的演化方式**；  
结论并非“被应用规则推出”，而是通过**结构匹配在形式上自然浮现**。

从这个意义上说，数学证明并不是在“使用语言描述推理”，  
而是在一个足够表达力的语言中，**让结构自行展开**。  
Lean 等形式化系统正是沿着这一思路设计的：  
证明不是叙述，而是结构的逐步对齐。

一个优秀的语言能够直接暴露系统的演化轨迹；  
在极端情况下，我们甚至难以区分：  
究竟是系统在演化，还是语言本身即是系统。

这并非审美差异，而是系统可操作性的差异。

因此，语言始终只是系统的符号表达层。  
针对不同需求，语言可以是强类型、依赖明确的；  
可以是弱约束、强调灵活性的；  
可以承载情感，也可以服务于过程描述与执行控制。  
不存在一种“全能语言”，能够同时优化所有维度。

更进一步说，语言的真正作用，是以符号的方式**定义、操作、甚至运行系统**，并将系统能力投射到可被理解与操控的层面。  
问题从来不在于语言是否足够强大，而在于我们是否清楚：  
**哪些语言适合操作哪种系统，每个系统究竟应该承担什么角色。**


## 问题不在模型，而在系统边界的失守

当前围绕 LLM 出现的大多数失败，并不源于模型是否“足够聪明”，
而是来自一个被默认为合理、却在工程上根本错误的前提：

只要系统能够用自然语言表达意图，就可以让自然语言直接接管执行。

在任何可靠的复杂系统中，至少存在三类本质不同的层级：

- 语言层
用于表达意图、假设与方案；允许不确定性、歧义、修正与错误。
自然语言、数学形式语言、编程语言都属于这一层——它们负责“说什么”，而不是“做什么”。

- 推理 / 检查层
位于抽象表达与实际执行之间，用于施加约束：语法检查、类型一致性、条件验证、错误定位与可审计性。
人类会要求澄清语义不清的指令；编译器会拒绝语法或类型不成立的程序；这一层的职责，是阻断错误的下沉。

- 执行层
不可逆、具有外部副作用，并天然绑定责任。
一旦触发，错误不再是信息噪声，而是现实事件。

这三层在可靠性、可逆性与失败代价上根本不同，
因此绝不能由同一机制同时承担。

然而，当下大量所谓的“Agent 系统”，
恰恰在用最不稳定、最不可审计的 LLM 黑箱，
将语言层通过弱语法、弱约束、稀疏滤网，
直接对接到最不可失败、最不可回滚的执行层。

问题不在于 LLM 会产生幻觉——
幻觉是语言补全机在弱约束下的内在属性；
真正的失败在于，系统设计者允许幻觉进入了不该进入的层级。

自然语言适合做什么？
提出方案、草拟内容、整理信息、探索可能性、进行开放式搜索与汇总。
它擅长的是生成“候选”，而不是触发“后果”。

自然语言并不是为操作系统而生的。
它的弱语法决定了：
即使一句话在语法和上下文上完全成立，也无法可靠地承担状态修改、资源调度或外部行为触发。
一旦语言越过自身能力边界，错误就不再是概率问题，而是结构性必然。

因此，基于自然语言的大语言模型的合理位置，从来不应是“自动执行者”。
它应当继承自然语言的通用性与表达力，用于翻译、提案、启发式搜索与信息组织；
但不应进入执行层，
更不应承担最终判断、验证与审计的角色。

自然语言本身不可 scale。
人类社会早已用“以讹传讹”“三人成虎”反复证明：
在缺乏形式化约束、推理校验与责任分配的情况下，
任何依赖语言放大的系统，都会系统性失真。

法律之所以能够规模化运作，并非因为语言本身可靠，
而是因为它使用了语法受限的表达方式，并由司法解释体系与人类法官共同兜底；
数学证明之所以可信，是因为形式系统的存在，以及被严格限定的适用范围。

语言从来不是自洽的。
它之所以能被信任，只因为它被限制在自身职责之内，并由外部约束与验证机制兜底。

当边界清晰，失败是可控的；
当边界模糊，再强的模型，也只会放大风险。

至此，所谓“智能”已经变成一个系统设计与架构的问题。


---

## 结语

我们是否能够跳出对语言本身的执念，对 LLM 的迷信，
转而正视系统架构、约束与审计？

今天的失速并不来自道路的错误，
而在于我们过于简化了对大型系统的理解——
我们在不断加固系统的单一组件：引擎与油门，却迟迟不肯面对一个事实：

**刹车与安全带，从来不是性能优化之后的可选项，
而是决定系统能否上路的前置条件。**

至于刹车与安全带是什么，且听下回分解。
