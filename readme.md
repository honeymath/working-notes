# The working notes 

## Postings:


[AI Runaway Is Not a Model Problem — It’s a Collapse of System Boundaries](posts/2026-01-04-ai-reasoning-english.md)
> In 2023, I first used ChatGPT to build my own AI agents: reading emails, sending replies, and scheduling events.
> I thought I was using “advanced AI.” What I actually found myself doing was repeatedly correcting an apprentice that kept making elementary mistakes. From that moment on, my most basic trust in AI began to erode.
> 
> Two years later, model capabilities have improved significantly, and obvious hallucinations have been greatly reduced. Even so, I still do not dare to let it send emails on my behalf.
> The problem is not whether the model is “smart enough,” but that when the boundaries between language, reasoning, and execution are systematically flattened, errors are no longer incidental—they become structural and inevitable.
> 
> If a system allows natural language to directly drive execution, then when errors occur:
> Who discovers them?
> Who stops them?
> Who bears responsibility for the consequences?
> 
> This article does not discuss whether AI is “aligned” or “safe.” Instead, from a system-architecture perspective grounded in basic engineering common sense, it asks several questions that are repeatedly ignored:
> 
> - What is the true nature of large language model capabilities? (Translation and completion, not reasoning.)
> - Why is the entire industry treating natural language as reasoning itself?
> - What roles is natural language suited for in a system, and what responsibilities should it never be allowed to assume?
> - Why are today’s popular agent architectures engineering the amplification of their own failure?
> 
> These questions all point to a single, avoided fact:
> Language is only the expression layer of a system. Safety and reliability depend on a separate reasoning/verification layer.
> 
> This is the first article in a two-part series. It focuses on the problem itself:
> Why treating natural language as a reasoning system—and directly connecting it to execution—is an engineering path destined to lose control.
> 
> The next article will address solutions:
> If language cannot, and should not, bear reasoning and judgment on its own, what should replace it? What would a verifiable, auditable, and accountable AI system architecture actually look like?
> 
> This is not a rejection of AI capability.
> It is a warning about a dangerous trend now spreading:
> treating human language as a substitute for reasoning and responsibility.



## About me

I’m Qirui Li. I’ve been writing code for over twenty years, and I’m trained as a mathematician, with a background in number theory and algebraic geometry.

I want to build my own AI by hand, but reality has repeatedly pushed back. What I write here comes from practice and failure notes, meant to avoid walking the same detours again and again.

Since AI technologies became widely used in 2023, I have watched social enthusiasm, misunderstanding, and hesitation—and how those misunderstandings, in turn, reshape reality. The absurdity of this process has driven me to record what I observe.

Many things look perfectly reasonable on the surface, yet feel deeply wrong when examined with basic common sense.

If you want to stay grounded and clear-headed, and to dismantle false narratives and hallucinations, you are welcome to subscribe.
